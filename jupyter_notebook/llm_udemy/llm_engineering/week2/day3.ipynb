{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-BMuGj\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyDW\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'\n",
    "docker_server_name=\"0.0.0.0\"\n",
    "\n",
    "import time\n",
    "import signal\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "def kill_process_on_port(port):\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'connections']):\n",
    "        try:\n",
    "            for conn in proc.info['connections']:\n",
    "                if conn.laddr.port == port:\n",
    "                    os.kill(proc.info['pid'], signal.SIGTERM)\n",
    "                    time.sleep(1)  # Give it time to terminate\n",
    "                    try:\n",
    "                        os.kill(proc.info['pid'], signal.SIGKILL)  # Force kill if still running\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'i want to run fast'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'i want to run fast', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Improving your running speed involves a combination of proper training, nutrition, and recovery. Here are some tips to help you run faster:\\n\\n### Training Tips:\\n\\n1. **Interval Training**: Incorporate high-intensity interval training (HIIT) into your routine. This involves alternating between short bursts of high-speed running and recovery periods.\\n\\n2. **Tempo Runs**: These runs help improve your lactate threshold. Run at a pace that is comfortably hard for a sustained period (20-30 minutes) after a warm-up.\\n\\n3. **Hill Workouts**: Running uphill increases strength and power in your legs, which can enhance speed. Find a hill and do repeats, running up at a hard pace and jogging or walking down for recovery.\\n\\n4. **Strength Training**: Focus on exercises that strengthen your legs, core, and overall body, such as lunges, squats, and deadlifts. Strong muscles can help with speed.\\n\\n5. **Proper Form**: Pay attention to your running form. Key aspects include keeping your head up, shoulders relaxed, and landing midfoot.\\n\\n6. **Fartlek Training**: Incorporate unstructured interval training in your runs. For example, sprint to a landmark, then jog to the next one, and repeat.\\n\\n### Nutritional Considerations:\\n\\n1. **Balanced Diet**: Ensure you're consuming a balanced diet with sufficient carbohydrates, proteins, and healthy fats to fuel your runs and aid recovery.\\n\\n2. **Hydration**: Stay hydrated before, during, and after your runs. Dehydration can significantly affect performance.\\n\\n3. **Pre-Workout Snacks**: Have a light snack before running, like a banana or a small energy bar, to boost your energy levels.\\n\\n### Recovery Strategies:\\n\\n1. **Rest Days**: Schedule regular rest days to allow your muscles to recover and adapt to the training.\\n\\n2. **Sleep**: Ensure youâ€™re getting enough quality sleep to support recovery and performance.\\n\\n3. **Foam Rolling and Stretching**: Use foam rolling and stretching to keep muscles flexible and reduce soreness.\\n\\n### Consistency and Goals:\\n\\n1. **Set Clear Goals**: Whether you're training for a race or just want to improve your speed, set specific, measurable goals.\\n\\n2. **Be Consistent**: Regularly follow your training schedule. Gradually increase intensity and volume to boost speed while avoiding injury.\\n\\n3. **Track Your Progress**: Use a running app or watch to track your running times and distances, which can help you see improvements over time.\\n\\n### Mental Training:\\n\\n1. **Positive Mindset**: Stay positive and visualize your goals. Mental confidence can boost performance.\\n\\n2. **Focus on Breathing**: Practice rhythmic breathing techniques to enhance your endurance during runs.\\n\\nRemember to listen to your body and avoid pushing too hard too quickly, as this can lead to injury. Good luck with your training!\", 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'i want to run fast', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Improving your running speed involves a combination of proper training, nutrition, and recovery. Here are some tips to help you run faster:\\n\\n### Training Tips:\\n\\n1. **Interval Training**: Incorporate high-intensity interval training (HIIT) into your routine. This involves alternating between short bursts of high-speed running and recovery periods.\\n\\n2. **Tempo Runs**: These runs help improve your lactate threshold. Run at a pace that is comfortably hard for a sustained period (20-30 minutes) after a warm-up.\\n\\n3. **Hill Workouts**: Running uphill increases strength and power in your legs, which can enhance speed. Find a hill and do repeats, running up at a hard pace and jogging or walking down for recovery.\\n\\n4. **Strength Training**: Focus on exercises that strengthen your legs, core, and overall body, such as lunges, squats, and deadlifts. Strong muscles can help with speed.\\n\\n5. **Proper Form**: Pay attention to your running form. Key aspects include keeping your head up, shoulders relaxed, and landing midfoot.\\n\\n6. **Fartlek Training**: Incorporate unstructured interval training in your runs. For example, sprint to a landmark, then jog to the next one, and repeat.\\n\\n### Nutritional Considerations:\\n\\n1. **Balanced Diet**: Ensure you're consuming a balanced diet with sufficient carbohydrates, proteins, and healthy fats to fuel your runs and aid recovery.\\n\\n2. **Hydration**: Stay hydrated before, during, and after your runs. Dehydration can significantly affect performance.\\n\\n3. **Pre-Workout Snacks**: Have a light snack before running, like a banana or a small energy bar, to boost your energy levels.\\n\\n### Recovery Strategies:\\n\\n1. **Rest Days**: Schedule regular rest days to allow your muscles to recover and adapt to the training.\\n\\n2. **Sleep**: Ensure youâ€™re getting enough quality sleep to support recovery and performance.\\n\\n3. **Foam Rolling and Stretching**: Use foam rolling and stretching to keep muscles flexible and reduce soreness.\\n\\n### Consistency and Goals:\\n\\n1. **Set Clear Goals**: Whether you're training for a race or just want to improve your speed, set specific, measurable goals.\\n\\n2. **Be Consistent**: Regularly follow your training schedule. Gradually increase intensity and volume to boost speed while avoiding injury.\\n\\n3. **Track Your Progress**: Use a running app or watch to track your running times and distances, which can help you see improvements over time.\\n\\n### Mental Training:\\n\\n1. **Positive Mindset**: Stay positive and visualize your goals. Mental confidence can boost performance.\\n\\n2. **Focus on Breathing**: Practice rhythmic breathing techniques to enhance your endurance during runs.\\n\\nRemember to listen to your body and avoid pushing too hard too quickly, as this can lead to injury. Good luck with your training!\", 'options': None}, {'role': 'user', 'content': 'but im getting old'}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Close existing instances\n",
    "gr.close_all()\n",
    "# Add a small delay to ensure port is released\n",
    "time.sleep(2)\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(server_name=docker_server_name, server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.close_all()\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(server_name=docker_server_name, server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gr\u001b[38;5;241m.\u001b[39mclose_all()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocker_server_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7860\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gradio/blocks.py:2562\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[1;32m   2557\u001b[0m     (\n\u001b[1;32m   2558\u001b[0m         server_name,\n\u001b[1;32m   2559\u001b[0m         server_port,\n\u001b[1;32m   2560\u001b[0m         local_url,\n\u001b[1;32m   2561\u001b[0m         server,\n\u001b[0;32m-> 2562\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m server_name\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url \u001b[38;5;241m=\u001b[39m local_url\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gradio/http_server.py:156\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     path_to_local_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "gr.close_all()\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(server_name=docker_server_name, server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# First close gracefully\n",
    "gr.close_all()\n",
    "time.sleep(2)\n",
    "\n",
    "# Then forcefully kill anything on port 7860\n",
    "kill_process_on_port(7860)\n",
    "'''\n",
    "# Find Python processes\n",
    "ps aux | grep python\n",
    "\n",
    "# Kill any existing Gradio processes\n",
    "pkill -f gradio\n",
    "'''\n",
    "time.sleep(2)\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(server_name=docker_server_name, server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
